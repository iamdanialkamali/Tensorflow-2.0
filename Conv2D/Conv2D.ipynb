{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Conv2D.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2J-vqcUkt2Wt",
        "colab_type": "text"
      },
      "source": [
        "# Conv2D layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wNUxMyJJqgeL",
        "colab_type": "text"
      },
      "source": [
        "## import libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eXYsPBNZnEqk",
        "colab_type": "code",
        "outputId": "8733ed51-d46e-4869-bc84-840cf2431c16",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "# Install TensorFlow\n",
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass\n",
        "\n",
        "import tensorflow as tf\n",
        "from pprint import pprint\n",
        "\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 2.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5nIRDyvYICQT",
        "colab_type": "text"
      },
      "source": [
        "## Defining Layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n6OZ_s4HIBy5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Put your answer here\n",
        "\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "\n",
        "class Donv2D(layers.Layer):\n",
        "\n",
        "  def conv2d(self,kernel_size,kernel_count,images):\n",
        "    if(len(images.shape) == 3):\n",
        "      images = tf.reshape(images,(images.shape[0],images.shape[1],images.shape[2],1)) ## in case of input image\n",
        "    else:\n",
        "      images = tf.reshape(images,(images.shape[0],images.shape[1],images.shape[2],images.shape[3])) ## in case of in between layers\n",
        "   \n",
        "    image_base = images.shape[0],images.shape[1]\n",
        "   \n",
        "    patches = tf.image.extract_patches(\n",
        "      images=images,\n",
        "      sizes=[1, kernel_size[0],kernel_size[1], 1],\n",
        "      strides=[1,1,1, 1],\n",
        "      rates=[1, 1, 1, 1],\n",
        "      padding='VALID'\n",
        "    )\n",
        "    \n",
        "    output_filters = (patches.shape[1],patches.shape[2])\n",
        "    patches = tf.reshape(patches,(patches.shape[0],patches.shape[1]*patches.shape[2],patches.shape[3]))\n",
        "    patches = tf.dtypes.cast(\n",
        "        patches,\n",
        "        tf.dtypes.float32,\n",
        "    )\n",
        "    patches= tf.transpose(patches,(0,2,1))\n",
        "    \n",
        "    conv = tf.matmul(self.filters,patches)\n",
        "    conv = tf.reshape(conv,(image_base[0],output_filters[0],output_filters[1],kernel_count))\n",
        "\n",
        "    return conv\n",
        "\n",
        "  def __init__(self, kernel_count,kernel_size,input_depth=32):\n",
        "    super(Donv2D, self).__init__()\n",
        "    self.kernel_count = kernel_count\n",
        "    self.kernel_size = kernel_size\n",
        "    self.valid = False\n",
        "    w_init = tf.random_normal_initializer()\n",
        "    self.filters = self.add_weight(shape=(kernel_count,kernel_size[0]*kernel_size[1]*input_depth),\n",
        "                             initializer='random_normal',\n",
        "                             trainable=True)\n",
        "    \n",
        "  def call(self, inputs):   \n",
        "      return self.conv2d(self.kernel_size,self.kernel_count,inputs)\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dRvpx1wYHatj",
        "colab_type": "text"
      },
      "source": [
        "## Defining Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_0UN3z_yHaSX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.datasets import mnist\n",
        "\n",
        "class Conv(tf.keras.Model):\n",
        "\n",
        "    def __init__(self,num_classes):\n",
        "        super(Conv, self).__init__()\n",
        "        self.block_1 = Donv2D(20,(3,3),1)\n",
        "        self.block_2 = Donv2D(32,(3,3),20)\n",
        "        self.block_3 = Donv2D(64,(3,3),32)\n",
        "        self.flatten = tf.keras.layers.Flatten()\n",
        "        self.global_pool = tf.keras.layers.GlobalAveragePooling2D()\n",
        "        self.classifier = tf.keras.layers.Dense(num_classes)\n",
        "        self.dense = tf.keras.layers.Dense(100)\n",
        "        self.sigmoid = tf.keras.layers.Activation('sigmoid')\n",
        "        self.relu = tf.keras.layers.Activation('relu')\n",
        "\n",
        "    def call(self, inputs):\n",
        "        x = self.block_1(inputs)\n",
        "        x = self.relu(x)\n",
        "        # print(x.shape)\n",
        "        # print(\"One is Done\")\n",
        "        x = self.block_2(x)\n",
        "        x = self.relu(x)\n",
        "        # print(x.shape)\n",
        "        # print(\"Two is Done\")\n",
        "        x = self.block_3(x)\n",
        "        x = self.relu(x)\n",
        "        # print(x.shape)\n",
        "        # print(\"Three is Done\")\n",
        "        x = self.global_pool(x)\n",
        "        # print(\"Connected\")\n",
        "        x = self.flatten(x)\n",
        "        x = self.relu(x)\n",
        "        # print(x.shape)\n",
        "        x = self.dense(x)\n",
        "        x = self.relu(x)\n",
        "\n",
        "        # print(x.shape)\n",
        "        x = self.classifier(x)\n",
        "        x = self.sigmoid(x)\n",
        "\n",
        "        # print(x.shape)\n",
        "\n",
        "\n",
        "        return x\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "9xD4Hfe0u1W9"
      },
      "source": [
        "## Problem 1\n",
        "Mnist Digit Recognition"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "K-5BZCNfu1XA"
      },
      "source": [
        "### import data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "gH4ndYHeu1XA",
        "outputId": "b81f6178-6033-4b48-ff92-f68dfce4224c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "(x_train, y_train),(x_test,y_test) = tf.keras.datasets.mnist.load_data()\n",
        "y_train = tf.keras.utils.to_categorical(y_train, num_classes=10)\n",
        "y_test = tf.keras.utils.to_categorical(y_test, num_classes=10)\n",
        "x_train.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000, 28, 28)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "4Ir5GY2Pu1XD"
      },
      "source": [
        "### visualization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oelIVMiZ7qLq",
        "colab_type": "code",
        "outputId": "8959fdd2-f50f-4a18-dfb8-0c673efbdfb7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        }
      },
      "source": [
        "plt.imshow(x_train[0])\n",
        "plt.title = y_train[0]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSRB2IAnCDiTx/65XcTNOWsh5AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q0iEoDJV8zCQ",
        "colab_type": "text"
      },
      "source": [
        "### Training Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mURdz9p08xNW",
        "colab_type": "code",
        "outputId": "d8cd51a9-66ad-4321-c0b6-1c016df1c100",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "conv = Conv(10)\n",
        "\n",
        "# Instantiate an optimizer.\n",
        "optimizer = tf.keras.optimizers.RMSprop(learning_rate=1e-3)\n",
        "# Instantiate a loss function.\n",
        "loss_fn = tf.keras.losses.CategoricalCrossentropy()\n",
        "acc =  tf.keras.metrics.CategoricalAccuracy()\n",
        "\n",
        "# Prepare the training dataset.\n",
        "batch_size = 64\n",
        "seen = True\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
        "train_dataset = train_dataset.shuffle(buffer_size=1024).batch(batch_size)\n",
        "epochs = 10\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  print('Start of epoch %d' % (epoch,))\n",
        "\n",
        "  # Iterate over the batches of the dataset.\n",
        "  for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n",
        "    # print(y_batch_train.shape)\n",
        "    with tf.GradientTape() as tape:\n",
        "      y_pred = conv(x_batch_train)   \n",
        "      loss_value = loss_fn(y_batch_train, y_pred)\n",
        "      a = acc(y_batch_train, y_pred)\n",
        "    grads = tape.gradient(loss_value, conv.trainable_weights)\n",
        "    optimizer.apply_gradients(zip(grads, conv.trainable_weights))\n",
        "\n",
        "    if step % 100 == 0:\n",
        "        print('Training loss (for one batch) at step %s: %s' % (step, float(loss_value)))\n",
        "        print('Training acc (for one batch) at step %s: %s' % (step, float(a)))\n",
        "        print('Seen so far: %s samples' % ((step + 1) * 64))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Start of epoch 0\n",
            "Training loss (for one batch) at step 0: 2.4747869968414307\n",
            "Training acc (for one batch) at step 0: 0.0625\n",
            "Seen so far: 64 samples\n",
            "Training loss (for one batch) at step 100: 0.9148193001747131\n",
            "Training acc (for one batch) at step 100: 0.6155630946159363\n",
            "Seen so far: 6464 samples\n",
            "Training loss (for one batch) at step 200: 0.3151840567588806\n",
            "Training acc (for one batch) at step 200: 0.7356964945793152\n",
            "Seen so far: 12864 samples\n",
            "Training loss (for one batch) at step 300: 0.3452046513557434\n",
            "Training acc (for one batch) at step 300: 0.7872197031974792\n",
            "Seen so far: 19264 samples\n",
            "Training loss (for one batch) at step 400: 0.17904913425445557\n",
            "Training acc (for one batch) at step 400: 0.8208385109901428\n",
            "Seen so far: 25664 samples\n",
            "Training loss (for one batch) at step 500: 0.19866925477981567\n",
            "Training acc (for one batch) at step 500: 0.8424713015556335\n",
            "Seen so far: 32064 samples\n",
            "Training loss (for one batch) at step 600: 0.19249111413955688\n",
            "Training acc (for one batch) at step 600: 0.8586470484733582\n",
            "Seen so far: 38464 samples\n",
            "Training loss (for one batch) at step 700: 0.20010143518447876\n",
            "Training acc (for one batch) at step 700: 0.8699848651885986\n",
            "Seen so far: 44864 samples\n",
            "Training loss (for one batch) at step 800: 0.09158486127853394\n",
            "Training acc (for one batch) at step 800: 0.8782771825790405\n",
            "Seen so far: 51264 samples\n",
            "Training loss (for one batch) at step 900: 0.10195309668779373\n",
            "Training acc (for one batch) at step 900: 0.8867924809455872\n",
            "Seen so far: 57664 samples\n",
            "Start of epoch 1\n",
            "Training loss (for one batch) at step 0: 0.08283492177724838\n",
            "Training acc (for one batch) at step 0: 0.8902004361152649\n",
            "Seen so far: 64 samples\n",
            "Training loss (for one batch) at step 100: 0.12081103026866913\n",
            "Training acc (for one batch) at step 100: 0.8967711925506592\n",
            "Seen so far: 6464 samples\n",
            "Training loss (for one batch) at step 200: 0.05446421355009079\n",
            "Training acc (for one batch) at step 200: 0.9020366668701172\n",
            "Seen so far: 12864 samples\n",
            "Training loss (for one batch) at step 300: 0.01716001331806183\n",
            "Training acc (for one batch) at step 300: 0.9067041873931885\n",
            "Seen so far: 19264 samples\n",
            "Training loss (for one batch) at step 400: 0.0460287407040596\n",
            "Training acc (for one batch) at step 400: 0.9109777808189392\n",
            "Seen so far: 25664 samples\n",
            "Training loss (for one batch) at step 500: 0.028502332046628\n",
            "Training acc (for one batch) at step 500: 0.9144399762153625\n",
            "Seen so far: 32064 samples\n",
            "Training loss (for one batch) at step 600: 0.07687141001224518\n",
            "Training acc (for one batch) at step 600: 0.9176450371742249\n",
            "Seen so far: 38464 samples\n",
            "Training loss (for one batch) at step 700: 0.05170559883117676\n",
            "Training acc (for one batch) at step 700: 0.9203253984451294\n",
            "Seen so far: 44864 samples\n",
            "Training loss (for one batch) at step 800: 0.17578867077827454\n",
            "Training acc (for one batch) at step 800: 0.9225265979766846\n",
            "Seen so far: 51264 samples\n",
            "Training loss (for one batch) at step 900: 0.0359458401799202\n",
            "Training acc (for one batch) at step 900: 0.9249132871627808\n",
            "Seen so far: 57664 samples\n",
            "Start of epoch 2\n",
            "Training loss (for one batch) at step 0: 0.06016453728079796\n",
            "Training acc (for one batch) at step 0: 0.9259728193283081\n",
            "Seen so far: 64 samples\n",
            "Training loss (for one batch) at step 100: 0.023505765944719315\n",
            "Training acc (for one batch) at step 100: 0.928169310092926\n",
            "Seen so far: 6464 samples\n",
            "Training loss (for one batch) at step 200: 0.11723443120718002\n",
            "Training acc (for one batch) at step 200: 0.9300487637519836\n",
            "Seen so far: 12864 samples\n",
            "Training loss (for one batch) at step 300: 0.031387344002723694\n",
            "Training acc (for one batch) at step 300: 0.931891918182373\n",
            "Seen so far: 19264 samples\n",
            "Training loss (for one batch) at step 400: 0.08950603753328323\n",
            "Training acc (for one batch) at step 400: 0.9336555600166321\n",
            "Seen so far: 25664 samples\n",
            "Training loss (for one batch) at step 500: 0.1682843267917633\n",
            "Training acc (for one batch) at step 500: 0.9352838397026062\n",
            "Seen so far: 32064 samples\n",
            "Training loss (for one batch) at step 600: 0.07134267687797546\n",
            "Training acc (for one batch) at step 600: 0.9368563294410706\n",
            "Seen so far: 38464 samples\n",
            "Training loss (for one batch) at step 700: 0.17884895205497742\n",
            "Training acc (for one batch) at step 700: 0.9381975531578064\n",
            "Seen so far: 44864 samples\n",
            "Training loss (for one batch) at step 800: 0.020642656832933426\n",
            "Training acc (for one batch) at step 800: 0.939350962638855\n",
            "Seen so far: 51264 samples\n",
            "Training loss (for one batch) at step 900: 0.06277105957269669\n",
            "Training acc (for one batch) at step 900: 0.9406013488769531\n",
            "Seen so far: 57664 samples\n",
            "Start of epoch 3\n",
            "Training loss (for one batch) at step 0: 0.3271031081676483\n",
            "Training acc (for one batch) at step 0: 0.9411431550979614\n",
            "Seen so far: 64 samples\n",
            "Training loss (for one batch) at step 100: 0.009884987957775593\n",
            "Training acc (for one batch) at step 100: 0.9423695802688599\n",
            "Seen so far: 6464 samples\n",
            "Training loss (for one batch) at step 200: 0.0861096903681755\n",
            "Training acc (for one batch) at step 200: 0.9434368014335632\n",
            "Seen so far: 12864 samples\n",
            "Training loss (for one batch) at step 300: 0.03412575647234917\n",
            "Training acc (for one batch) at step 300: 0.9445208311080933\n",
            "Seen so far: 19264 samples\n",
            "Training loss (for one batch) at step 400: 0.01749270409345627\n",
            "Training acc (for one batch) at step 400: 0.9455762505531311\n",
            "Seen so far: 25664 samples\n",
            "Training loss (for one batch) at step 500: 0.05833020061254501\n",
            "Training acc (for one batch) at step 500: 0.9464878439903259\n",
            "Seen so far: 32064 samples\n",
            "Training loss (for one batch) at step 600: 0.014883903786540031\n",
            "Training acc (for one batch) at step 600: 0.9473872184753418\n",
            "Seen so far: 38464 samples\n",
            "Training loss (for one batch) at step 700: 0.12477491050958633\n",
            "Training acc (for one batch) at step 700: 0.9482131600379944\n",
            "Seen so far: 44864 samples\n",
            "Training loss (for one batch) at step 800: 0.016410809010267258\n",
            "Training acc (for one batch) at step 800: 0.9490020275115967\n",
            "Seen so far: 51264 samples\n",
            "Training loss (for one batch) at step 900: 0.04509979113936424\n",
            "Training acc (for one batch) at step 900: 0.9497988820075989\n",
            "Seen so far: 57664 samples\n",
            "Start of epoch 4\n",
            "Training loss (for one batch) at step 0: 0.011778784915804863\n",
            "Training acc (for one batch) at step 0: 0.9501549601554871\n",
            "Seen so far: 64 samples\n",
            "Training loss (for one batch) at step 100: 0.10376890003681183\n",
            "Training acc (for one batch) at step 100: 0.9508894085884094\n",
            "Seen so far: 6464 samples\n",
            "Training loss (for one batch) at step 200: 0.010521911084651947\n",
            "Training acc (for one batch) at step 200: 0.9516459703445435\n",
            "Seen so far: 12864 samples\n",
            "Training loss (for one batch) at step 300: 0.0976869985461235\n",
            "Training acc (for one batch) at step 300: 0.9523227214813232\n",
            "Seen so far: 19264 samples\n",
            "Training loss (for one batch) at step 400: 0.029451124370098114\n",
            "Training acc (for one batch) at step 400: 0.9530082941055298\n",
            "Seen so far: 25664 samples\n",
            "Training loss (for one batch) at step 500: 0.010705593042075634\n",
            "Training acc (for one batch) at step 500: 0.9537351727485657\n",
            "Seen so far: 32064 samples\n",
            "Training loss (for one batch) at step 600: 0.10438527166843414\n",
            "Training acc (for one batch) at step 600: 0.9543962478637695\n",
            "Seen so far: 38464 samples\n",
            "Training loss (for one batch) at step 700: 0.07084573805332184\n",
            "Training acc (for one batch) at step 700: 0.9549960494041443\n",
            "Seen so far: 44864 samples\n",
            "Training loss (for one batch) at step 800: 0.011644931510090828\n",
            "Training acc (for one batch) at step 800: 0.9554905295372009\n",
            "Seen so far: 51264 samples\n",
            "Training loss (for one batch) at step 900: 0.10777392983436584\n",
            "Training acc (for one batch) at step 900: 0.9560443758964539\n",
            "Seen so far: 57664 samples\n",
            "Start of epoch 5\n",
            "Training loss (for one batch) at step 0: 0.2930106222629547\n",
            "Training acc (for one batch) at step 0: 0.9562993049621582\n",
            "Seen so far: 64 samples\n",
            "Training loss (for one batch) at step 100: 0.014714164659380913\n",
            "Training acc (for one batch) at step 100: 0.9568562507629395\n",
            "Seen so far: 6464 samples\n",
            "Training loss (for one batch) at step 200: 0.03289126604795456\n",
            "Training acc (for one batch) at step 200: 0.9573521018028259\n",
            "Seen so far: 12864 samples\n",
            "Training loss (for one batch) at step 300: 0.1736689805984497\n",
            "Training acc (for one batch) at step 300: 0.9578405618667603\n",
            "Seen so far: 19264 samples\n",
            "Training loss (for one batch) at step 400: 0.10588037967681885\n",
            "Training acc (for one batch) at step 400: 0.9583712220191956\n",
            "Seen so far: 25664 samples\n",
            "Training loss (for one batch) at step 500: 0.07402394711971283\n",
            "Training acc (for one batch) at step 500: 0.9588512778282166\n",
            "Seen so far: 32064 samples\n",
            "Training loss (for one batch) at step 600: 0.024070311337709427\n",
            "Training acc (for one batch) at step 600: 0.9593664407730103\n",
            "Seen so far: 38464 samples\n",
            "Training loss (for one batch) at step 700: 0.02970535308122635\n",
            "Training acc (for one batch) at step 700: 0.9598015546798706\n",
            "Seen so far: 44864 samples\n",
            "Training loss (for one batch) at step 800: 0.03170570731163025\n",
            "Training acc (for one batch) at step 800: 0.9601752758026123\n",
            "Seen so far: 51264 samples\n",
            "Training loss (for one batch) at step 900: 0.05106952413916588\n",
            "Training acc (for one batch) at step 900: 0.9606027007102966\n",
            "Seen so far: 57664 samples\n",
            "Start of epoch 6\n",
            "Training loss (for one batch) at step 0: 0.12608285248279572\n",
            "Training acc (for one batch) at step 0: 0.9608014225959778\n",
            "Seen so far: 64 samples\n",
            "Training loss (for one batch) at step 100: 0.15533384680747986\n",
            "Training acc (for one batch) at step 100: 0.9611939787864685\n",
            "Seen so far: 6464 samples\n",
            "Training loss (for one batch) at step 200: 0.031242195516824722\n",
            "Training acc (for one batch) at step 200: 0.9616509079933167\n",
            "Seen so far: 12864 samples\n",
            "Training loss (for one batch) at step 300: 0.1262103021144867\n",
            "Training acc (for one batch) at step 300: 0.9620844721794128\n",
            "Seen so far: 19264 samples\n",
            "Training loss (for one batch) at step 400: 0.07261557877063751\n",
            "Training acc (for one batch) at step 400: 0.9624906778335571\n",
            "Seen so far: 25664 samples\n",
            "Training loss (for one batch) at step 500: 0.022258084267377853\n",
            "Training acc (for one batch) at step 500: 0.962868332862854\n",
            "Seen so far: 32064 samples\n",
            "Training loss (for one batch) at step 600: 0.19884449243545532\n",
            "Training acc (for one batch) at step 600: 0.9632488489151001\n",
            "Seen so far: 38464 samples\n",
            "Training loss (for one batch) at step 700: 0.059015851467847824\n",
            "Training acc (for one batch) at step 700: 0.9636075496673584\n",
            "Seen so far: 44864 samples\n",
            "Training loss (for one batch) at step 800: 0.009861024096608162\n",
            "Training acc (for one batch) at step 800: 0.9639550447463989\n",
            "Seen so far: 51264 samples\n",
            "Training loss (for one batch) at step 900: 0.02905181236565113\n",
            "Training acc (for one batch) at step 900: 0.9643469452857971\n",
            "Seen so far: 57664 samples\n",
            "Start of epoch 7\n",
            "Training loss (for one batch) at step 0: 0.2512659728527069\n",
            "Training acc (for one batch) at step 0: 0.9644768238067627\n",
            "Seen so far: 64 samples\n",
            "Training loss (for one batch) at step 100: 0.013335241936147213\n",
            "Training acc (for one batch) at step 100: 0.9648387432098389\n",
            "Seen so far: 6464 samples\n",
            "Training loss (for one batch) at step 200: 0.03970073163509369\n",
            "Training acc (for one batch) at step 200: 0.9651807546615601\n",
            "Seen so far: 12864 samples\n",
            "Training loss (for one batch) at step 300: 0.03974173590540886\n",
            "Training acc (for one batch) at step 300: 0.965496838092804\n",
            "Seen so far: 19264 samples\n",
            "Training loss (for one batch) at step 400: 0.058757681399583817\n",
            "Training acc (for one batch) at step 400: 0.9658218026161194\n",
            "Seen so far: 25664 samples\n",
            "Training loss (for one batch) at step 500: 0.0025968223344534636\n",
            "Training acc (for one batch) at step 500: 0.9661375284194946\n",
            "Seen so far: 32064 samples\n",
            "Training loss (for one batch) at step 600: 0.025393841788172722\n",
            "Training acc (for one batch) at step 600: 0.9664204716682434\n",
            "Seen so far: 38464 samples\n",
            "Training loss (for one batch) at step 700: 0.005815902724862099\n",
            "Training acc (for one batch) at step 700: 0.9667128324508667\n",
            "Seen so far: 44864 samples\n",
            "Training loss (for one batch) at step 800: 0.05530892312526703\n",
            "Training acc (for one batch) at step 800: 0.9669612050056458\n",
            "Seen so far: 51264 samples\n",
            "Training loss (for one batch) at step 900: 0.002984172198921442\n",
            "Training acc (for one batch) at step 900: 0.9672510623931885\n",
            "Seen so far: 57664 samples\n",
            "Start of epoch 8\n",
            "Training loss (for one batch) at step 0: 0.0009516647551208735\n",
            "Training acc (for one batch) at step 0: 0.9673751592636108\n",
            "Seen so far: 64 samples\n",
            "Training loss (for one batch) at step 100: 0.0074845259077847\n",
            "Training acc (for one batch) at step 100: 0.9676440358161926\n",
            "Seen so far: 6464 samples\n",
            "Training loss (for one batch) at step 200: 0.0045386748388409615\n",
            "Training acc (for one batch) at step 200: 0.9679445028305054\n",
            "Seen so far: 12864 samples\n",
            "Training loss (for one batch) at step 300: 0.025977719575166702\n",
            "Training acc (for one batch) at step 300: 0.968227207660675\n",
            "Seen so far: 19264 samples\n",
            "Training loss (for one batch) at step 400: 0.011723874136805534\n",
            "Training acc (for one batch) at step 400: 0.968504786491394\n",
            "Seen so far: 25664 samples\n",
            "Training loss (for one batch) at step 500: 0.021272752434015274\n",
            "Training acc (for one batch) at step 500: 0.9687578082084656\n",
            "Seen so far: 32064 samples\n",
            "Training loss (for one batch) at step 600: 0.0551300123333931\n",
            "Training acc (for one batch) at step 600: 0.9689949750900269\n",
            "Seen so far: 38464 samples\n",
            "Training loss (for one batch) at step 700: 0.045192983001470566\n",
            "Training acc (for one batch) at step 700: 0.9692111015319824\n",
            "Seen so far: 44864 samples\n",
            "Training loss (for one batch) at step 800: 0.008501145988702774\n",
            "Training acc (for one batch) at step 800: 0.9694314002990723\n",
            "Seen so far: 51264 samples\n",
            "Training loss (for one batch) at step 900: 0.00011530512711033225\n",
            "Training acc (for one batch) at step 900: 0.9696688055992126\n",
            "Seen so far: 57664 samples\n",
            "Start of epoch 9\n",
            "Training loss (for one batch) at step 0: 0.007957170717418194\n",
            "Training acc (for one batch) at step 0: 0.9697684049606323\n",
            "Seen so far: 64 samples\n",
            "Training loss (for one batch) at step 100: 0.00420426158234477\n",
            "Training acc (for one batch) at step 100: 0.9699687361717224\n",
            "Seen so far: 6464 samples\n",
            "Training loss (for one batch) at step 200: 0.011408806778490543\n",
            "Training acc (for one batch) at step 200: 0.970206081867218\n",
            "Seen so far: 12864 samples\n",
            "Training loss (for one batch) at step 300: 0.04517115280032158\n",
            "Training acc (for one batch) at step 300: 0.9704325795173645\n",
            "Seen so far: 19264 samples\n",
            "Training loss (for one batch) at step 400: 0.03688410297036171\n",
            "Training acc (for one batch) at step 400: 0.9706663489341736\n",
            "Seen so far: 25664 samples\n",
            "Training loss (for one batch) at step 500: 0.010057760402560234\n",
            "Training acc (for one batch) at step 500: 0.9708983898162842\n",
            "Seen so far: 32064 samples\n",
            "Training loss (for one batch) at step 600: 0.0012681849766522646\n",
            "Training acc (for one batch) at step 600: 0.9710630178451538\n",
            "Seen so far: 38464 samples\n",
            "Training loss (for one batch) at step 700: 0.031731873750686646\n",
            "Training acc (for one batch) at step 700: 0.9712548851966858\n",
            "Seen so far: 44864 samples\n",
            "Training loss (for one batch) at step 800: 0.007447856478393078\n",
            "Training acc (for one batch) at step 800: 0.9714374542236328\n",
            "Seen so far: 51264 samples\n",
            "Training loss (for one batch) at step 900: 0.0392756387591362\n",
            "Training acc (for one batch) at step 900: 0.9716362357139587\n",
            "Seen so far: 57664 samples\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "92_yTMew9B-7",
        "colab_type": "text"
      },
      "source": [
        "### Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G6FufWtu9BcR",
        "colab_type": "code",
        "outputId": "1474d852-1eca-4631-b14a-0f6cc7002071",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "y_pred = conv(x_test)  # Logits for this \n",
        "a = acc(y_test, y_pred)\n",
        "print(a)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(0.9718164, shape=(), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XcLW2_xiH-rL",
        "colab_type": "text"
      },
      "source": [
        "Accuracy is %97.18 "
      ]
    }
  ]
}